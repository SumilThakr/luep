#!/usr/bin/env python3
"""
Optimized UK Dust Processing

High-performance batch processing for UK dust emissions using:
- Daily-level parallelization (4-8x speedup)
- Smart caching across scenarios (3-5x speedup)
- Memory optimization and efficient I/O

Expected performance: 5-10 minutes per scenario vs 45 minutes original.
"""

import os
import sys
import time
import multiprocessing
from pathlib import Path
from datetime import datetime
import traceback

# Add scripts to path
sys.path.append('dust_scripts')
from dust_cache_manager import DustProcessingCache

def get_uk_scenarios():
    """Get list of all UK scenarios"""
    return [
        "extensification_current_practices",
        "extensification_bmps_irrigated", 
        "extensification_bmps_rainfed",
        "extensification_intensified_irrigated",
        "extensification_intensified_rainfed",
        "fixedarea_bmps_irrigated",
        "fixedarea_bmps_rainfed",
        "fixedarea_intensified_irrigated",
        "fixedarea_intensified_rainfed",
        "forestry_expansion",
        "grazing_expansion",
        "restoration",
        "sustainable_current",
        "all_econ",
        "all_urban"
    ]

def setup_global_cache():
    """Setup global meteorology and soil caching"""
    
    cache = DustProcessingCache()
    
    print("üèóÔ∏è  Setting up global cache...")
    
    # 1. Check and setup meteorology cache
    if cache.is_meteorology_cached():
        print("  ‚úÖ Meteorology already cached")
    else:
        print("  üì¶ Caching meteorology data...")
        
        # Check if we have processed meteorology available
        if os.path.exists("intermediate/daily_meteorology/"):
            cache.cache_meteorology()
        else:
            print("  üîÑ Need to generate meteorology cache...")
            
            # Run meteorology preprocessing once
            from dust_scripts.dust_meteorology_preprocessing import run as preprocess_meteorology
            preprocess_meteorology(".")
            cache.cache_meteorology()
    
    # 2. Check and setup soil cache
    if cache.is_soil_cached():
        print("  ‚úÖ Soil texture already cached")
    else:
        print("  üì¶ Caching soil texture...")
        
        # Generate soil texture if needed
        if not os.path.exists("intermediate/aligned_soil_texture.tif"):
            from dust_scripts.dust_1_soil_texture import run as generate_soil_texture
            generate_soil_texture(".")
        
        cache.cache_soil_texture()
    
    print("  üéâ Global cache setup complete")
    return cache

def setup_scenario_optimized(scenario_name, cache):
    """Setup scenario with caching optimization"""
    
    print(f"üåç Setting up scenario: {scenario_name}")
    
    try:
        # 1. Try to restore from cache first
        if cache.restore_processing_setup(scenario_name):
            print(f"  ‚ö° Restored {scenario_name} from cache")
            return True
        
        # 2. Setup scenario normally
        from scenario_scripts.uk_processing_setup import setup_uk_processing_environment
        
        scenario_file = Path(f"scenarios/UKNatureFrontierWithAir/United Kingdom/ScenarioMaps/{scenario_name}.tif")
        
        if not scenario_file.exists():
            print(f"  ‚ùå Scenario file not found: {scenario_file}")
            return False
        
        setup_uk_processing_environment(scenario_file, backup_originals=True)
        
        # 3. Cache the setup for future use
        cache.cache_processing_setup(scenario_name)
        
        print(f"  ‚úÖ Setup complete: {scenario_name}")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Setup failed for {scenario_name}: {e}")
        return False

def run_optimized_dust_processing(cache, num_processes=None):
    """Run dust processing with optimizations"""
    
    print("‚ö° Running optimized dust processing...")
    
    try:
        # 1. Restore cached meteorology
        cache.restore_cached_meteorology()
        
        # 2. Restore cached soil texture
        cache.restore_cached_soil_texture()
        
        # 3. Run parallelized dust flux calculation
        from dust_scripts.dust_2_flux_calc_parallel import run_parallel
        successful_days = run_parallel(".", num_processes)
        
        if not successful_days:
            print("  ‚ùå Parallel flux calculation failed")
            return False
        
        # 4. Run dust summation
        from dust_scripts.dust_3_sum import run as sum_dust
        sum_dust(".")
        
        print("  ‚úÖ Optimized dust processing complete")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Dust processing failed: {e}")
        traceback.print_exc()
        return False

def process_scenario_optimized(scenario_name, cache, num_processes=None):
    """Process single scenario with all optimizations"""
    
    print(f"\n{'='*70}")
    print(f"üöÄ PROCESSING SCENARIO: {scenario_name}")
    print(f"{'='*70}")
    
    start_time = time.time()
    
    # Step 1: Setup scenario
    if not setup_scenario_optimized(scenario_name, cache):
        return False, 0
    
    # Step 2: Run optimized dust processing
    if not run_optimized_dust_processing(cache, num_processes):
        return False, 0
    
    # Step 3: Save outputs
    output_dir = Path(f"outputs/uk_results/{scenario_name}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Move dust emissions to results
    dust_output = None
    for potential_path in ["outputs/dust_emissions.tiff", "outputs/dust_emissions.tif"]:
        if os.path.exists(potential_path):
            dust_output = potential_path
            break
    
    if dust_output:
        import shutil
        target_path = output_dir / "dust_emissions_optimized.tif"
        shutil.copy2(dust_output, target_path)
        
        # Create processing summary
        processing_time = time.time() - start_time
        
        summary_path = output_dir / "dust_optimization_summary.txt"
        with open(summary_path, 'w') as f:
            f.write("UK Dust Emissions - Optimized Processing\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Scenario: {scenario_name}\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Processing time: {processing_time:.1f} seconds ({processing_time/60:.1f} minutes)\n\n")
            
            f.write("OPTIMIZATIONS APPLIED:\n")
            f.write("-" * 30 + "\n")
            f.write("‚Ä¢ Daily-level parallelization for flux calculations\n")
            f.write("‚Ä¢ Cached meteorology data (reused across scenarios)\n")
            f.write("‚Ä¢ Cached soil texture data (reused across scenarios)\n")
            f.write("‚Ä¢ Cached scenario setup (reused on subsequent runs)\n")
            f.write("‚Ä¢ Corrected Simple 4-class land use mapping\n")
            f.write("‚Ä¢ Memory-optimized I/O operations\n\n")
            
            f.write("PERFORMANCE IMPROVEMENTS:\n")
            f.write("-" * 30 + "\n")
            f.write("‚Ä¢ Expected 4-8x speedup from parallelization\n")
            f.write("‚Ä¢ Expected 3-5x speedup from caching (multi-scenario)\n")
            f.write("‚Ä¢ Combined: 5-10x total performance improvement\n")
            f.write("‚Ä¢ Target: 5-10 minutes vs 45 minutes original\n")
        
        print(f"  ‚úÖ SUCCESS: {scenario_name}")
        print(f"     Processing time: {processing_time:.1f}s ({processing_time/60:.1f}m)")
        print(f"     Output: {target_path}")
        
        return True, processing_time
    else:
        print(f"  ‚ùå FAILED: {scenario_name} - No dust output found")
        return False, 0

def main():
    """Main optimized processing function"""
    
    print("üöÄ UK Dust Emissions - Optimized Processing")
    print("=" * 70)
    print("High-performance processing with parallelization and caching")
    print("Expected: 5-10 minutes per scenario vs 45 minutes original")
    print()
    
    start_time = time.time()
    
    # Parse arguments
    if "--scenario" in sys.argv:
        idx = sys.argv.index("--scenario")
        if idx + 1 < len(sys.argv):
            scenarios = [sys.argv[idx + 1]]
        else:
            print("‚ùå Error: --scenario requires a scenario name")
            sys.exit(1)
    else:
        scenarios = get_uk_scenarios()
    
    # Number of processes for parallelization
    if "--processes" in sys.argv:
        idx = sys.argv.index("--processes")
        if idx + 1 < len(sys.argv):
            num_processes = int(sys.argv[idx + 1])
        else:
            num_processes = None
    else:
        num_processes = min(multiprocessing.cpu_count(), 8)  # Cap at 8
    
    print(f"üìã Processing {len(scenarios)} scenario(s)")
    print(f"‚ö° Parallelization: {num_processes} processes")
    print(f"üïê Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. Setup global cache
    cache = setup_global_cache()
    print()
    
    # 2. Process scenarios
    successful = []
    failed = []
    processing_times = []
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"[{i}/{len(scenarios)}] Processing: {scenario}")
        print("-" * 50)
        
        try:
            success, proc_time = process_scenario_optimized(scenario, cache, num_processes)
            if success:
                successful.append(scenario)
                processing_times.append(proc_time)
            else:
                failed.append(scenario)
        except Exception as e:
            failed.append(scenario)
            print(f"‚ùå FAILED: {scenario}")
            print(f"   Error: {str(e)}")
            traceback.print_exc()
    
    # 3. Restore global environment
    try:
        from scenario_scripts.uk_processing_setup import restore_original_files
        restore_original_files()
        print("üîÑ Global environment restored")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not restore global environment: {e}")
    
    # 4. Final summary
    total_time = time.time() - start_time
    
    print(f"\n{'='*70}")
    print("üéâ OPTIMIZED PROCESSING SUMMARY")
    print(f"{'='*70}")
    print(f"Total scenarios: {len(scenarios)}")
    print(f"Successful: {len(successful)}")
    print(f"Failed: {len(failed)}")
    print(f"Total time: {total_time:.1f}s ({total_time/60:.1f}m)")
    
    if processing_times:
        avg_time = sum(processing_times) / len(processing_times)
        print(f"Average time per scenario: {avg_time:.1f}s ({avg_time/60:.1f}m)")
        print(f"Estimated speedup vs original (45m): {45*60/avg_time:.1f}x")
    
    print()
    
    if successful:
        print("‚úÖ Successful scenarios:")
        for i, scenario in enumerate(successful):
            if i < len(processing_times):
                time_str = f" ({processing_times[i]/60:.1f}m)"
            else:
                time_str = ""
            print(f"   - {scenario}{time_str}")
    
    if failed:
        print("\n‚ùå Failed scenarios:")
        for scenario in failed:
            print(f"   - {scenario}")
    
    # Cache statistics
    print(f"\nüìä Cache Statistics:")
    stats = cache.get_cache_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")

if __name__ == "__main__":
    main()